{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91753c26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Framing DataFrames\n",
    "\n",
    "### Pyowa - 2025-05-27\n",
    "### Adam Best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc522ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "\n",
    "- 6 years of experience as a Data Engineer - 10 years with data\n",
    "- John Deere Engine Works - Quality Engineering\n",
    "- General Dynamics IT - Healthcare Data Warehouse\n",
    "- Dwolla - Data Platform/Warehouse + AWS\n",
    "- John Deere Financial - Database Administration + AWS\n",
    "- Tractor Zoom - Data Platform + AWS + Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4b080",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goals\n",
    "- Understanding of when and how to use a DataFrame\n",
    "- Cover some of the options, and what tools might be best for your use case\n",
    "- Some deeper dives into Daft as an example library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2cf65",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Will not cover\n",
    "- Exact syntax of each library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb6b2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is a DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d94bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What is a DataFrame?\n",
    "- Tabular data structure with rows and columns (2D)\n",
    "- Each column has a consistent data type (homogeneous)\n",
    "- Columns are aligned and indexed for efficient access\n",
    "- Supports operations on entire columns (vectorized)\n",
    "- Provides built-in methods for data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daceb41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Common tasks\n",
    "\n",
    "- Selecting and filtering\n",
    "- Aggregations - sum, count\n",
    "- Joining or merging\n",
    "- Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489298d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ways to create a DataFrame\n",
    "- Python `list` of `dict`s\n",
    "- Python `dict` of `list`s\n",
    "- NumPy arrays (pandas)\n",
    "- Files (csv, json, parquet, excel, etc.)\n",
    "- SQL Database `pandas.read_sql`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f78c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Essential Tools - Inspecting Data\n",
    "- `head(n)` and `tail(n)` - first and last n rows\n",
    "- `info()` - consise summary of the DataFrame, including data types and missing values\n",
    "- `describe()` - statistical summary of numeric columns\n",
    "- `shape` - number of rows and columns\n",
    "- `columns` - list of column names\n",
    "- `dtypes` - data types of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7330ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selection and Filtering\n",
    "- Column selection - `df['col1']`, `df[['col1', 'col2']]`, `df.col1`\n",
    "- Row selection by integer position (pandas) - `df.iloc[0]`, `df.iloc[[0, 1, 2]]`, `df.iloc[:, 0]`\n",
    "- Boolean indexing (filtering) - `df[df['column_name'] > some_value]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b8610",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Missing Values\n",
    "- `.isnull()` - check for missing values\n",
    "- `.dropna()` - remove rows or columns with missing values\n",
    "- `.fillna(value)` - replace missing values with a specific value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68905a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Column Operations\n",
    "- `df['new_column_name'] = df['existing_column'] * 2` - add a new column\n",
    "- `df['column_name'] = df['column_name'] + 5` - modify an existing column\n",
    "- `del df['column_name']` or `df.drop(columns=['column_name'])` - remove a column\n",
    "- `df.rename(columns={'old_name': 'new_name'})` - rename a column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fa77d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sorting, Grouping, and Aggregations\n",
    "- `df.sort_values('column_name')`\n",
    "    - sort by a column\n",
    "- `df.groupby('category_column')['value_column'].mean()`\n",
    "    - average of value_col for each unique value in category_column\n",
    "- `df.groupby('category_column')['value_column'].agg(['mean', 'sum', 'count'])`\n",
    "    - multiple aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df36bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Combining DataFrames\n",
    "- `df1.join(df2, on='common_column', how='inner')`\n",
    "    - merge two DataFrames on a common column\n",
    "- `df1.concat(df2)`\n",
    "    - concatenate two DataFrames vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b7b05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "from pandas import DataFrame\n",
    "\n",
    "df = DataFrame({\n",
    "    'name': ['John', 'Jane', 'Jim', 'Jill'],\n",
    "    'age': [20, 21, 22, 23],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dbb214",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What isn't a DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c0acc1f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# List of Dicts\n",
    "data = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n",
    "# No indexing or operations on fields\n",
    "# e.g. fetch all records with age > 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229ba647",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Dict of List\n",
    "data = {\"name\": [\"Alice\", \"Bob\"], \"age\": [30, 25]}\n",
    "# Column based, but lacks type and shape enforcement\n",
    "# Similar to how Dataframes are usually structured conceptually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ecf592",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Named Tuple + Dataclasses (closer)\n",
    "from collections import namedtuple\n",
    "\n",
    "Person = namedtuple(\"Person\", [\"name\", \"age\"])\n",
    "people = [Person(\"Alice\", 30), Person(\"Bob\", 25)]\n",
    "# again, no operations on specific fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3f0fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SQL Tables\n",
    "```sql\n",
    "CREATE TABLE people\n",
    "(id int,\n",
    " name, varchar(30),\n",
    " age, int);\n",
    "\n",
    "SELECT COUNT(*) FROM people WHERE age > 30;\n",
    "```\n",
    "- Very similar functionality to DataFrames\n",
    "- Declarative, lacks reusability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91b9ae6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "Each of these represents structured or semi-structured data, but only DataFrames combine labeling, tabular structure, and column operations in one unified tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc2ebb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So you want to use a DataFrame\n",
    "# ðŸ¤”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39583341",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# The options (some of them)\n",
    "- Pandas\n",
    "- PySpark\n",
    "- Dask\n",
    "- Polars\n",
    "- Daft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788a207",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pandas\n",
    "[Docs](https://pandas.pydata.org/docs/reference/index.html)\n",
    "- ~2012\n",
    "- huge userbase\n",
    "- mature and flexible\n",
    "- single threaded, memory constrained\n",
    "- Cython/C based\n",
    "- Seamless integration with NumPy, Matplotlib/Seaborn (for plotting), and Scikit-learn (for machine learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7676bd3e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dummy_data = {\n",
    "    \"name\": [\"John\", \"Jane\", \"Jim\", \"Jill\"],\n",
    "    \"age\": [20, 21, 22, 23],\n",
    "    \"city\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc0729",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df = pd.DataFrame(dummy_data)\n",
    "\n",
    "avg_age_by_city_pandas = pd_df.groupby(\"city\")[\"age\"].mean()\n",
    "avg_age_by_city_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aae67c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark\n",
    "[Docs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html)\n",
    "- ~2014\n",
    "- Massively horizontally scalable\n",
    "- SQL syntax support\n",
    "- high overhead for small datasets\n",
    "- JVM - Scala based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85ca1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrameDemo\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"city\", StringType())\n",
    "])\n",
    "\n",
    "spk_df = spark.createDataFrame(pd_df,  # Note: Pandas DataFrame source\n",
    "                               schema=schema)\n",
    "\n",
    "avg_age_by_city_spark = spk_df.groupBy(\"city\").mean(\"age\")\n",
    "avg_age_by_city_spark.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bbc736",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dask\n",
    "[Docs](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "- ~2015\n",
    "- Parallel and lazy evaluation\n",
    "- Similar to Pandas API\n",
    "- Handles larger than memory datasets\n",
    "- Python based\n",
    "- Delegates to other libraries for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28b815",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "dask_df = dd.from_dict(dummy_data, npartitions=2)\n",
    "avg_age_by_city_dask = dask_df.groupby(\"city\")[\"age\"].mean()\n",
    "avg_age_by_city_dask.compute()  # Note: necessary to trigger computation\n",
    "avg_age_by_city_dask.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd716d5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Polars\n",
    "[Docs](https://docs.pola.rs/user-guide/getting-started/)\n",
    "- ~2021\n",
    "- Fast - parallel and lazy evaluation\n",
    "- Less flexible for edge cases\n",
    "- Rust based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba887523",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "pl_df = pl.DataFrame(dummy_data)\n",
    "\n",
    "avg_age_by_city_polars = pl_df.group_by(\"city\").agg(pl.col(\"age\").mean())\n",
    "avg_age_by_city_polars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a3db3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Daft\n",
    "[Docs](https://www.getdaft.io/projects/docs/en/stable/quickstart/)\n",
    "- ~2023\n",
    "- Python + Rust + SQL\n",
    "- Scales Vertically and Horizontally (with Ray)\n",
    "- Cloud Storage Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc990738",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import daft\n",
    "\n",
    "daft_df = daft.from_pydict(dummy_data)\n",
    "avg_age_by_city_daft = daft_df.groupby(\"city\").agg(daft.col(\"age\").mean())\n",
    "\n",
    "avg_age_by_city_daft.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30b505",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Side by side\n",
    "pd_df.groupby(\"city\")[\"age\"].mean()  # Pandas\n",
    "spk_df.groupBy(\"city\").mean(\"age\")  # PySpark\n",
    "dask_df.groupby(\"city\")[\"age\"].mean()  # Dask\n",
    "pl_df.group_by(\"city\").agg(pl.col(\"age\").mean())  # Polars\n",
    "daft_df.groupby(\"city\").agg(daft.col(\"age\").mean())  # Daft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe722f86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Standards](./resources/standards.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a134c0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Enter Ibis (and others)\n",
    "[Docs](https://ibis-project.org/tutorials/basics)\n",
    "> Ibis defines a Python dataframe API that executes on any query engine â€“ the frontend for any backend data platform, with nearly 20 backends today. This allows Ibis to have excellent performance â€“ as good as the backend it is connected to â€“ with a consistent user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc5b0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modin\n",
    "[Docs](https://modin.readthedocs.io/en/latest/getting_started/why_modin/modin_vs_dask_vs_koalas.html)\n",
    "> Libraries such as Dask DataFrame (DaskDF for short) and Koalas aim to support the pandas API on top of distributed computing frameworks, Dask and Spark respectively. Instead, Modin aims to preserve the pandas API and behavior as is, while abstracting away the details of the distributed computing framework underneath. Thus, the aims of these libraries are fundamentally different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb2409",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More Modin\n",
    "> Specifically, Modin\n",
    "    - enables pandas-like row and column-parallel operations, unlike DaskDF and Koalas that only support row-parallel operations\n",
    "    - indexing & ordering semantics, unlike DaskDF and Koalas that deviate from these semantics\n",
    "    - eager execution, unlike DaskDF and Koalas that provide lazy execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d2e24b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparing\n",
    "\n",
    "| Feature/Need             | pandas | Polars | Dask | PySpark | Daft  |\n",
    "|--------------------------|--------|--------|------|---------|-------|\n",
    "| Easy local work          | âœ…     | âœ…     | âš ï¸   | âš ï¸      | âœ…    |\n",
    "| Huge files (10M+ rows)   | âš ï¸     | âœ…     | âœ…   | âœ…      | âœ…    |\n",
    "| Multi-core               | âŒ     | âœ…     | âœ…   | âœ…      | âœ…    |\n",
    "| Clustered / distributed  | âŒ     | âš ï¸     | âœ…   | âœ…      | âœ…    |\n",
    "| SQL-like syntax          | âš ï¸     | âš ï¸     | âš ï¸   | âœ…      | âš ï¸    |\n",
    "| Learning curve           | Easy   | Medium | Medium | Steep  | Medium |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0616964",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another Comparison table (from Daft docs)\n",
    "![Table](./resources/comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed3d43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quick Benchmarks\n",
    "[Link](./resources/benchmarks.html)\n",
    "![Benchmarks](./resources/benchmarks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e2c6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get our hands dirty\n",
    "import daft\n",
    "\n",
    "taxi_data = \"../resources/2023_Green_Taxi_Trip_Data_20250514.csv\"\n",
    "df = daft.read_csv(taxi_data).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30595567",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# sample it\n",
    "df.show(3)\n",
    "df.count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066dc7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# how many trips had a distance > 10 miles and > 1 passenger?\n",
    "df = df.with_column(\"is_long_multi\", \n",
    "                   (df[\"trip_distance\"] > 10) & (df[\"passenger_count\"] > 1))\n",
    "\n",
    "percent_count = (df.groupby(\"is_long_multi\")\n",
    "                 .agg(daft.col(\"VendorID\")\n",
    "                      .count()\n",
    "                      .alias(\"count\"))\n",
    "                      .collect())\n",
    "percent_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3e095",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Why None?\n",
    "(df.select(df[\"is_long_multi\"], df[\"trip_distance\"], df[\"passenger_count\"])\n",
    "    .where(df[\"is_long_multi\"].is_null())\n",
    "    .show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3683a4a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# how about nested object records?\n",
    "d = {\n",
    "    \"timestamp\": \"2025-01-02T02:03:04.000Z\",\n",
    "    \"id\": 123486993,\n",
    "    \"obj\": {\n",
    "        \"key\": \"my_key\",\n",
    "        \"value\": \"my_value\"\n",
    "    }\n",
    "}\n",
    "# works from_json() as well\n",
    "df = daft.from_pydict({\"log\": [d]}).collect()\n",
    "\n",
    "(df.filter(df[\"log\"][\"obj\"][\"value\"] == 'my_value')\n",
    " .select(df[\"log\"][\"obj\"][\"key\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2166e18e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example of lazy execution\n",
    "large_df = daft.read_csv(taxi_data)\n",
    "result = (\n",
    "    large_df.filter(large_df[\"total_amount\"] > 500.00)  # Filter first\n",
    "    .select(large_df[\"DOLocationID\"], large_df[\"total_amount\"])  # Then select only needed columns\n",
    "    .sort(\"total_amount\", desc=True)  # Order by amount\n",
    "    .limit(5)  # Finally take just 5 rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157caeeb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# only now are results computed\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c675f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Joining datasets\n",
    "taxi_df = daft.read_csv(taxi_data)\n",
    "locations_df = daft.from_pydict({\"location_id\": [145, 265]})\n",
    "\n",
    "joined_df = taxi_df.join(\n",
    "    locations_df, left_on=\"DOLocationID\", right_on=\"location_id\", how=\"inner\"\n",
    ").collect()\n",
    "\n",
    "joined_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805553d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Daft specific features\n",
    "- Every library is different, built for different use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d35d135",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Live Query Over Public S3 Parquet File\n",
    "Read directly from an S3-hosted dataset with no explicit download step.\n",
    "> The total file size is around 37 gigabytes, even in the efficient Parquet file format. Thatâ€™s bigger than memory on most peopleâ€™s computers, so you canâ€™t just read it all in and stack it into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27119930",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import daft\n",
    "\n",
    "df = daft.read_parquet(\"s3://ursa-labs-taxi-data/2019/*/data.parquet\")\n",
    "filtered = df.where(df[\"passenger_count\"] > 4)\n",
    "filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e112cfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lazy Filtering and Optimized Execution Plan\n",
    "Filters are not executed until necessary and are optimized into a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407bb05d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = daft.read_csv(\"../resources/large_dataset.csv\")\n",
    "# For demonstration, we use a small generated DataFrame\n",
    "df = daft.from_pydict({\"col1\": list(range(100)), \"col2\": [\"NY\"] * 50 + [\"CA\"] * 50})\n",
    "filtered = df.where(df[\"col1\"] > 10).where(df[\"col2\"] == \"NY\")\n",
    "filtered.explain(show_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789bd0a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualize Execution DAG\n",
    "Another example with a smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415fe59",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = daft.from_pydict({\"val\": list(range(100)), \"category\": [\"A\"] * 50 + [\"B\"] * 50})\n",
    "\n",
    "result = df.where(df[\"val\"] > 10).groupby(\"category\").agg(df[\"val\"].mean())\n",
    "\n",
    "result.explain(show_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1190cf1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manual Repartitioning for Optimized Joins\n",
    "Control the physical layout of your data to speed up joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7640852",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df1 = daft.from_pydict({\"user_id\": list(range(100)), \"score\": list(range(100))})\n",
    "df2 = daft.from_pydict({\"user_id\": list(range(50, 150)), \"label\": [\"active\"] * 100})\n",
    "\n",
    "partitioned_df1 = df1.repartition(None, \"user_id\")\n",
    "partitioned_df2 = df2.repartition(None, \"user_id\")\n",
    "partitioned_joined = partitioned_df1.join(partitioned_df2, on=\"user_id\")\n",
    "partitioned_joined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b9723",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Write to a Delta table\n",
    "- Provides:\n",
    "    - ACID transactions\n",
    "    - Scalable metadata\n",
    "    - Interoperability with other data tools\n",
    "    - History and recovery (Time Travel)\n",
    "    - Schema enforcement and evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fd424",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example (install Daft deltalake module)\n",
    "import daft\n",
    "\n",
    "df = daft.read_csv(\"../resources/large_dataset.csv\")\n",
    "df.write_deltalake(\"../resources/delta_table\", mode=\"overwrite\")\n",
    "# Read from a Delta table\n",
    "delta_df = daft.read_deltalake(\"../resources/delta_table\")\n",
    "delta_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c7610",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Try writing a different dataset to that table:\n",
    "taxi_df = daft.read_csv(taxi_data)\n",
    "taxi_df.write_deltalake(\"../resources/delta_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605fda9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyArrow vs Parquet\n",
    "- Parquet - disk based columnar storage\n",
    "- PyArrow - in memory columnar storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b69c0",
   "metadata": {},
   "source": [
    "# Wrapping up\n",
    "- Every library is different\n",
    "- Each has tradeoffs\n",
    "- Makes scalable python data analysis easier\n",
    "- Daft is a good fit for many use cases\n",
    "- Disclaimer: I'm not affiliated with Daft, I just think they're cool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b23e2",
   "metadata": {},
   "source": [
    "# Resource Links/Recommended Reading:\n",
    "- [Parquet vs Arrow](https://medium.com/@diehardankush/comparing-data-storage-parquet-vs-arrow-aa2231e51c8a)\n",
    "- [Understanding Vectorization](https://medium.com/analytics-vidhya/understanding-vectorization-in-numpy-and-pandas-188b6ebc5398)\n",
    "- [This presentation code](https://github.com/adam133/pyowa-dataframes)\n",
    "- [Daft docs](https://www.getdaft.io/projects/docs/en/stable/)\n",
    "- [Pandas docs](https://pandas.pydata.org/docs/reference/index.html)\n",
    "- [Polars docs](https://docs.pola.rs/)\n",
    "- [Dask docs](https://docs.dask.org/en/stable/)\n",
    "- [PySpark docs](https://spark.apache.org/docs/latest/api/python/reference/index.html)\n",
    "- LinkedIn: [Adam Best](https://www.linkedin.com/in/adamrbest)\n",
    "- Active in the [PyOwa Discord](https://discord.gg/KSsEVdVpGZ)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "rise": {
   "progress": true,
   "slideNumber": true,
   "theme": "night"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
