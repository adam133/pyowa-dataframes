{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91753c26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Framing DataFrames\n",
    "\n",
    "### Pyowa - 2025-05-27\n",
    "### Adam Best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc522ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "\n",
    "- 6 years of experience as a Data Engineer - 10 years with data\n",
    "- John Deere Engine Works - Quality Engineering\n",
    "- General Dynamics IT - Healthcare Data Warehouse\n",
    "- Dwolla - Data Platform/Warehouse + AWS\n",
    "- John Deere Financial - Database Administration + AWS\n",
    "- Tractor Zoom - Data Platform + AWS + Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4b080",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goals\n",
    "- Understanding of when and how to use a DataFrame\n",
    "- Cover some of the options, and what tools might be best for your use case\n",
    "- Some deeper dives into Daft as an example library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2cf65",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "### Will not cover\n",
    "- Exact syntax of each library\n",
    "- How to build a data pipeline\n",
    "- Machine learning/AI/Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640a2c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stirring the data\n",
    "![AI](https://imgs.xkcd.com/comics/machine_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb6b2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is a DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d94bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What is a DataFrame?\n",
    "- Tabular data structure with rows and columns (2D)\n",
    "- Each column has a consistent data type (homogeneous)\n",
    "- Columns are aligned and indexed for efficient access\n",
    "- Supports operations on entire columns (vectorized)\n",
    "- Provides built-in methods for data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07f679",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Emphasize - vectorized operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daceb41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Common tasks\n",
    "\n",
    "- Selecting and filtering\n",
    "- Aggregations - sum, count\n",
    "- Joining or merging\n",
    "- Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b489298d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ways to create a DataFrame\n",
    "- Python `list` of `dict`s\n",
    "- Python `dict` of `list`s\n",
    "- NumPy arrays (pandas)\n",
    "- Files (csv, json, parquet, excel, etc.)\n",
    "- SQL Database `pandas.read_sql`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f78c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Essential Tools - Inspecting Data\n",
    "- `head(n)` and `tail(n)` - first and last n rows\n",
    "- `info()` - consise summary of the DataFrame, including data types and missing values\n",
    "- `describe()` - statistical summary of numeric columns\n",
    "- `shape` - number of rows and columns\n",
    "- `columns` - list of column names\n",
    "- `dtypes` - data types of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7330ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selection and Filtering\n",
    "- Column selection - `df['col1']`, `df[['col1', 'col2']]`, `df.col1`\n",
    "- Row selection by integer position (pandas) - `df.iloc[0]`, `df.iloc[[0, 1, 2]]`, `df.iloc[:, 0]`\n",
    "- Boolean indexing (filtering) - `df[df['column_name'] > some_value]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b8610",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Missing Values\n",
    "- `.isnull()` - check for missing values\n",
    "- `.dropna()` - remove rows or columns with missing values\n",
    "- `.fillna(value)` - replace missing values with a specific value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68905a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Column Operations\n",
    "- `df['new_column_name'] = df['existing_column'] * 2` - add a new column\n",
    "- `df['column_name'] = df['column_name'] + 5` - modify an existing column\n",
    "- `del df['column_name']` or `df.drop(columns=['column_name'])` - remove a column\n",
    "- `df.rename(columns={'old_name': 'new_name'})` - rename a column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fa77d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sorting, Grouping, and Aggregations\n",
    "- `df.sort_values('column_name')`\n",
    "    - sort by a column\n",
    "- `df.groupby('category_column')['value_column'].mean()`\n",
    "    - average of value_col for each unique value in category_column\n",
    "- `df.groupby('category_column')['value_column'].agg(['mean', 'sum', 'count'])`\n",
    "    - multiple aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df36bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Combining DataFrames\n",
    "- `df1.join(df2, on='common_column', how='inner')`\n",
    "    - merge two DataFrames on a common column\n",
    "- `df1.concat(df2)`\n",
    "    - concatenate two DataFrames vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b7b05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "from pandas import DataFrame\n",
    "\n",
    "df = DataFrame({\n",
    "    'name': ['John', 'Jane', 'Jim', 'Jill'],\n",
    "    'age': [20, 21, 22, 23],\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dbb214",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What isn't a DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0acc1f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# List of Dicts\n",
    "data = [{\"name\": \"Alice\", \"age\": 30}, {\"name\": \"Bob\", \"age\": 25}]\n",
    "# No indexing or operations on fields\n",
    "# e.g. fetch all records with age > 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ba647",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Dict of List\n",
    "data = {\"name\": [\"Alice\", \"Bob\"], \"age\": [30, 25]}\n",
    "# Column based, but lacks type and shape enforcement\n",
    "# Similar to how Dataframes are usually structured conceptually, lacks vectorized operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecf592",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Named Tuple + Dataclasses (closer)\n",
    "from collections import namedtuple\n",
    "\n",
    "Person = namedtuple(\"Person\", [\"name\", \"age\"])\n",
    "people = [Person(\"Alice\", 30), Person(\"Bob\", 25)]\n",
    "# again, no operations on specific fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3f0fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SQL Tables\n",
    "```sql\n",
    "CREATE TABLE people\n",
    "(id int,\n",
    " name, varchar(30),\n",
    " age, int);\n",
    "\n",
    "SELECT COUNT(*) FROM people WHERE age > 30;\n",
    "```\n",
    "- Very similar functionality to DataFrames\n",
    "- Declarative, lacks reusability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e7781",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A note on Vectorization\n",
    "![Vectorize it](./resources/vectorize_it.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a820eab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv('../resources/large_dataset.csv')\n",
    "\n",
    "before_v = datetime.now()\n",
    "# ... Vectorized operation:\n",
    "df[\"ratio\"] = 100 * (df[\"zip\"] / df[\"id\"])\n",
    "after_v = datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a14ae17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# ... Non-vectorized operation: (DON'T DO THIS)\n",
    "def calc_ratio(row):\n",
    "    return 100 * (row[\"zip\"] / row[\"id\"])\n",
    "\n",
    "before_row = datetime.now()\n",
    "df[\"ratio2\"] = df.apply(calc_ratio, axis=1)\n",
    "after_row = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf22c5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Vectorized time: {(after_v - before_v).total_seconds():.4f} seconds\")\n",
    "print(f\"Non-vectorized time: {(after_row - before_row).total_seconds():.4f} seconds\")\n",
    "print(\n",
    "    f\"Speedup: {(after_row - before_row).total_seconds() / (after_v - before_v).total_seconds():.2f}x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91b9ae6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "Each of these represents structured or semi-structured data, but only DataFrames combine labeling, tabular structure, and column operations in one unified tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab5edf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When to use a DataFrame\n",
    "- Working with hundreds or thousands of rows/columns\n",
    "- Performance is important\n",
    "- Need to scale\n",
    "- Ok to install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ada67",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "On dependencies: This repo has 117\n",
    "\n",
    "`uv pip freeze | wc -l`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3adfb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When not to use a DataFrame\n",
    "- Working with a few rows/columns, not expected to grow\n",
    "- Unable to install dependencies\n",
    "- \"Excel is fine\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc2ebb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So you want to use a DataFrame\n",
    "![Choices](./resources/choices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39583341",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The options (some of them)\n",
    "- Pandas\n",
    "- PySpark\n",
    "- Dask\n",
    "- Polars\n",
    "- Daft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788a207",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pandas\n",
    "[Docs](https://pandas.pydata.org/docs/reference/index.html)\n",
    "- ~2012\n",
    "- huge userbase\n",
    "- mature and flexible\n",
    "- single threaded, memory constrained\n",
    "- Cython/C based\n",
    "- Seamless integration with NumPy, Matplotlib/Seaborn (for plotting), and Scikit-learn (for machine learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7676bd3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dummy_data = {\n",
    "    \"name\": [\"John\", \"Jane\", \"Jim\", \"Jill\"],\n",
    "    \"age\": [20, 21, 22, 23],\n",
    "    \"city\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc0729",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df = pd.DataFrame(dummy_data)\n",
    "\n",
    "avg_age_by_city_pandas = pd_df.groupby(\"city\")[\"age\"].mean()\n",
    "avg_age_by_city_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aae67c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PySpark\n",
    "[Docs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html)\n",
    "- ~2010\n",
    "- Massively horizontally scalable\n",
    "- Excellent SQL syntax support\n",
    "- high overhead for small datasets\n",
    "- JVM - Scala based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777a712",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Emphasis on large number of compute nodes\n",
    "\n",
    "lineage of data and fault tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85ca1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrameDemo\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"city\", StringType())\n",
    "])\n",
    "\n",
    "spk_df = spark.createDataFrame(pd_df,  # Note: Pandas DataFrame source\n",
    "                               schema=schema)\n",
    "\n",
    "avg_age_by_city_spark = spk_df.groupBy(\"city\").mean(\"age\")\n",
    "avg_age_by_city_spark.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bbc736",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dask\n",
    "[Docs](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "- ~2015\n",
    "- Parallel and lazy evaluation\n",
    "- Similar to Pandas API\n",
    "- Handles larger than memory datasets\n",
    "- Python based\n",
    "- Delegates to other libraries for execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23efe4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A layer on top of Pandas to address the single threaded nature of Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28b815",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "dask_df = dd.from_dict(dummy_data, npartitions=2)\n",
    "avg_age_by_city_dask = dask_df.groupby(\"city\")[\"age\"].mean()\n",
    "avg_age_by_city_dask.compute()  # Note: necessary to trigger computation\n",
    "avg_age_by_city_dask.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd716d5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Polars\n",
    "[Docs](https://docs.pola.rs/user-guide/getting-started/)\n",
    "- ~2021\n",
    "- Fast - parallel and lazy evaluation\n",
    "- Less flexible for edge cases\n",
    "- Rust based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba887523",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "pl_df = pl.DataFrame(dummy_data)\n",
    "\n",
    "avg_age_by_city_polars = pl_df.group_by(\"city\").agg(pl.col(\"age\").mean())\n",
    "avg_age_by_city_polars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a3db3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Daft\n",
    "[Docs](https://www.getdaft.io/projects/docs/en/stable/quickstart/)\n",
    "- ~2023\n",
    "- Python + Rust + SQL\n",
    "- Scales Vertically and Horizontally (with Ray)\n",
    "- Cloud Storage Optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11fc82",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Ray - Open source distributed execution engine\n",
    "\n",
    "Eventual Inc backed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc990738",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import daft\n",
    "\n",
    "daft_df = daft.from_pydict(dummy_data)\n",
    "avg_age_by_city_daft = daft_df.groupby(\"city\").agg(daft.col(\"age\").mean())\n",
    "\n",
    "avg_age_by_city_daft.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30b505",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Side by side\n",
    "pd_df.groupby(\"city\")[\"age\"].mean()  # Pandas\n",
    "spk_df.groupBy(\"city\").mean(\"age\")  # PySpark\n",
    "dask_df.groupby(\"city\")[\"age\"].mean()  # Dask\n",
    "pl_df.group_by(\"city\").agg(pl.col(\"age\").mean())  # Polars\n",
    "daft_df.groupby(\"city\").agg(daft.col(\"age\").mean())  # Daft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e52b8",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Pandas <> Dask very similar\n",
    "\n",
    "Polars <> Daft very similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe722f86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Standards](https://imgs.xkcd.com/comics/standards.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a134c0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Enter Ibis (and others)\n",
    "[Docs](https://ibis-project.org/tutorials/basics)\n",
    "> Ibis defines a Python dataframe API that executes on any query engine – the frontend for any backend data platform, with nearly 20 backends today. This allows Ibis to have excellent performance – as good as the backend it is connected to – with a consistent user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc5b0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modin\n",
    "[Docs](https://modin.readthedocs.io/en/latest/getting_started/why_modin/modin_vs_dask_vs_koalas.html)\n",
    "> Libraries such as Dask DataFrame (DaskDF for short) and Koalas aim to support the pandas API on top of distributed computing frameworks, Dask and Spark respectively. Instead, Modin aims to preserve the pandas API and behavior as is, while abstracting away the details of the distributed computing framework underneath. Thus, the aims of these libraries are fundamentally different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd949f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Koalas - now included in PySpark (Pandas API on Spark)\n",
    "\n",
    "```python\n",
    "import pyspark.pandas as ps\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb2409",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More Modin\n",
    "> Specifically, Modin\n",
    "    - enables pandas-like row and column-parallel operations, unlike DaskDF and Koalas that only support row-parallel operations\n",
    "    - indexing & ordering semantics, unlike DaskDF and Koalas that deviate from these semantics\n",
    "    - eager execution, unlike DaskDF and Koalas that provide lazy execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d2e24b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparing\n",
    "\n",
    "| Feature/Need             | pandas | Polars | Dask | PySpark | Daft  |\n",
    "|--------------------------|--------|--------|------|---------|-------|\n",
    "| Easy local work          | ✅     | ✅     | ⚠️   | ⚠️      | ✅    |\n",
    "| Huge files (10M+ rows)   | ⚠️     | ✅     | ✅   | ✅      | ✅    |\n",
    "| Multi-core               | ❌     | ✅     | ✅   | ✅      | ✅    |\n",
    "| Clustered / distributed  | ❌     | ⚠️     | ✅   | ✅      | ✅    |\n",
    "| SQL-like syntax          | ⚠️     | ⚠️     | ⚠️   | ✅      | ⚠️    |\n",
    "| Learning curve           | Easy   | Medium | Medium | Steep  | Medium |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0616964",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another Comparison table (from Daft docs)\n",
    "![Table](./resources/comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605fda9c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# PyArrow vs Parquet\n",
    "- Parquet - disk based columnar storage\n",
    "- PyArrow - in memory columnar storage\n",
    "    - Multi language support, super efficient in memory OLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed3d43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quick Benchmarks\n",
    "![Benchmarks](./resources/benchmarks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89cffa",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Dask - slower than pandas, but uses less memory\n",
    "\n",
    "Polars - very fast\n",
    "\n",
    "Daft - not as fast as Polars, higher overhead\n",
    "\n",
    "PySpark - Possible issue with benchmark methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e2c6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get our hands dirty\n",
    "import daft\n",
    "\n",
    "taxi_data = \"../resources/2023_Green_Taxi_Trip_Data_20250514.csv\"\n",
    "df = daft.read_csv(taxi_data).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30595567",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# sample it\n",
    "df.show(3)\n",
    "df.count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066dc7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# how many trips had a distance > 10 miles and > 1 passenger?\n",
    "df = df.with_column(\"is_long_multi\", \n",
    "                   (df[\"trip_distance\"] > 10) & (df[\"passenger_count\"] > 1))\n",
    "\n",
    "percent_count = (df.groupby(\"is_long_multi\")\n",
    "                 .agg(daft.col(\"VendorID\")\n",
    "                      .count()\n",
    "                      .alias(\"count\"))\n",
    "                      .collect())\n",
    "percent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3e095",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Why None?\n",
    "(df.select(df[\"is_long_multi\"], df[\"trip_distance\"], df[\"passenger_count\"])\n",
    "    .where(df[\"is_long_multi\"].is_null())\n",
    "    .show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3683a4a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# how about nested object records?\n",
    "d = {\n",
    "    \"timestamp\": \"2025-01-02T02:03:04.000Z\",\n",
    "    \"id\": 123486993,\n",
    "    \"obj\": {\n",
    "        \"key\": \"my_key\",\n",
    "        \"value\": \"my_value\"\n",
    "    }\n",
    "}\n",
    "# works from_json() as well\n",
    "df = daft.from_pydict({\"log\": [d]}).collect()\n",
    "\n",
    "(df.filter(df[\"log\"][\"obj\"][\"value\"] == 'my_value')\n",
    " .select(df[\"log\"][\"obj\"][\"key\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166e18e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example of lazy execution\n",
    "large_df = daft.read_csv(taxi_data)\n",
    "result = (\n",
    "    large_df.filter(large_df[\"total_amount\"] > 500.00)  # Filter first\n",
    "    .select(large_df[\"DOLocationID\"], large_df[\"total_amount\"])  # Then select only needed columns\n",
    "    .sort(\"total_amount\", desc=True)  # Order by amount\n",
    "    .limit(5)  # Finally take just 5 rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157caeeb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# only now are results computed\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c675f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Joining datasets\n",
    "taxi_df = daft.read_csv(taxi_data)\n",
    "locations_df = daft.from_pydict({\"location_id\": [145, 265]})\n",
    "\n",
    "joined_df = taxi_df.join(\n",
    "    locations_df, left_on=\"DOLocationID\", right_on=\"location_id\", how=\"inner\"\n",
    ").collect()\n",
    "\n",
    "joined_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805553d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Daft specific features\n",
    "- Every library is different, built for different use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d35d135",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Live Query Over Public S3 Parquet File\n",
    "Read directly from an S3-hosted dataset with no explicit download step.\n",
    "> The total file size is around 37 gigabytes, even in the efficient Parquet file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27119930",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import daft\n",
    "\n",
    "s3_df = daft.read_parquet(\"s3://ursa-labs-taxi-data/2019/*/data.parquet\")\n",
    "filtered = s3_df.where(s3_df[\"passenger_count\"] > 4)\n",
    "filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e112cfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lazy Filtering and Optimized Execution Plan\n",
    "Filters are not executed until necessary and are optimized into a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407bb05d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = daft.read_csv(\"../resources/large_dataset.csv\")\n",
    "# For demonstration, we use a small generated DataFrame\n",
    "df = daft.from_pydict({\"col1\": list(range(100)), \"col2\": [\"NY\"] * 50 + [\"CA\"] * 50})\n",
    "filtered = df.where(df[\"col1\"] > 10).where(df[\"col2\"] == \"NY\")\n",
    "filtered.explain(show_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789bd0a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualize Execution DAG\n",
    "Another example with a smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415fe59",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = daft.from_pydict({\"val\": list(range(100)), \"category\": [\"A\"] * 50 + [\"B\"] * 50})\n",
    "\n",
    "result = df.where(df[\"val\"] > 10).groupby(\"category\").agg(df[\"val\"].mean())\n",
    "\n",
    "result.explain(show_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617d6d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# s3 scan example explain\n",
    "filtered.explain(show_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1190cf1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manual Repartitioning for Optimized Joins\n",
    "Control the physical layout of your data to speed up joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7640852",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df1 = daft.from_pydict({\"user_id\": list(range(100)), \"score\": list(range(100))})\n",
    "df2 = daft.from_pydict({\"user_id\": list(range(50, 150)), \"label\": [\"active\"] * 100})\n",
    "\n",
    "partitioned_df1 = df1.repartition(None, \"user_id\")\n",
    "partitioned_df2 = df2.repartition(None, \"user_id\")\n",
    "partitioned_joined = partitioned_df1.join(partitioned_df2, on=\"user_id\")\n",
    "partitioned_joined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b6da5a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Partitioning ensures related data is colocated\n",
    "\n",
    "Typically should be handled by the optimizer, can be a useful performance optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b9723",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Write to a Delta table\n",
    "- Provides:\n",
    "    - ACID transactions\n",
    "        - Atomicity, Consistency, Isolation, Durability\n",
    "    - Scalable metadata\n",
    "    - Interoperability with other data tools\n",
    "    - History and recovery (Time Travel)\n",
    "    - Schema enforcement and evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f7ed3",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Basis of Unity Catalog\n",
    "\n",
    "Comparable to Apache Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fd424",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example (install Daft deltalake module)\n",
    "import daft\n",
    "\n",
    "df = daft.read_csv(\"../resources/large_dataset.csv\")\n",
    "df.write_deltalake(\"../resources/delta_table\", mode=\"overwrite\")\n",
    "# Read from a Delta table\n",
    "delta_df = daft.read_deltalake(\"../resources/delta_table\")\n",
    "delta_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c7610",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Try writing a different dataset to that table:\n",
    "taxi_df = daft.read_csv(\"../resources/2023_Green_Taxi_Trip_Data_20250514.csv\")\n",
    "taxi_df.write_deltalake(\"../resources/delta_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9a4db",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Very popular in the data lake space\n",
    "\n",
    "Watch out for small files, can cause performance issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b69c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrapping up\n",
    "- Every library is different\n",
    "- Each has tradeoffs\n",
    "- Makes scalable python data analysis easier\n",
    "- Daft is a good fit for many use cases\n",
    "- Disclaimer: I'm not affiliated with Daft, I just think they're cool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b23e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resource Links/Recommended Reading:\n",
    "- [Parquet vs Arrow](https://medium.com/@diehardankush/comparing-data-storage-parquet-vs-arrow-aa2231e51c8a)\n",
    "- [Understanding Vectorization](https://medium.com/analytics-vidhya/understanding-vectorization-in-numpy-and-pandas-188b6ebc5398)\n",
    "- [This presentation code](https://github.com/adam133/pyowa-dataframes)\n",
    "- [Daft docs](https://www.getdaft.io/projects/docs/en/stable/)\n",
    "- [Pandas docs](https://pandas.pydata.org/docs/reference/index.html)\n",
    "- [Polars docs](https://docs.pola.rs/)\n",
    "- [Dask docs](https://docs.dask.org/en/stable/)\n",
    "- [PySpark docs](https://spark.apache.org/docs/latest/api/python/reference/index.html)\n",
    "- LinkedIn: [Adam Best](https://www.linkedin.com/in/adamrbest)\n",
    "- Active in the [PyOwa Discord](https://discord.gg/KSsEVdVpGZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abffe03",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Thanks to PyOwa and Source Allies for hosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e55af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?\n",
    "![End](./resources/end.jpg)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "rise": {
   "progress": true,
   "scroll": true,
   "slideNumber": true,
   "theme": "moon"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
